{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n","metadata":{}},{"cell_type":"markdown","source":"\nCredit : https://www.kaggle.com/code/chengskin/dnn-model-ensemble-of-ubiquant\n\nThe majority of the content of this notebook comes from the original author. The contribution that has been made is in the choice of weights in the submission function (the last cell). You can improve it and get a higher score than 0.1554.\n\nGood experimentation to you\n\nIf this notebook is useful to you, please DO UPVOTE","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport gc\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow import keras\nimport mlcrate as mlc\nimport pickle as pkl\nfrom tensorflow.keras.layers import BatchNormalization\nfrom keras.models import Sequential, Model\nfrom keras.layers import Input, Embedding, Dense, Flatten, Concatenate, Dot, Reshape, Add, Subtract\nfrom keras import backend as K\nfrom keras import regularizers \nfrom tensorflow.keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.regularizers import l2\nfrom sklearn.base import clone\nfrom typing import Dict\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom tensorflow.keras.losses import Loss\nfrom tensorflow.keras import backend as K\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import TimeSeriesSplit, StratifiedKFold, KFold, GroupKFold\nfrom tqdm import tqdm\nfrom tensorflow.python.ops import math_ops","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-07T15:48:04.205896Z","iopub.execute_input":"2022-04-07T15:48:04.206550Z","iopub.status.idle":"2022-04-07T15:48:10.866587Z","shell.execute_reply.started":"2022-04-07T15:48:04.206460Z","shell.execute_reply":"2022-04-07T15:48:10.865680Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"%%time\nn_features = 300\nfeatures = [f'f_{i}' for i in range(n_features)]\nfeature_columns = ['investment_id', 'time_id'] + features\ntrain = pd.read_pickle('../input/ubiquant-market-prediction-half-precision-pickle/train.pkl')\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-07T15:48:10.867964Z","iopub.execute_input":"2022-04-07T15:48:10.868317Z","iopub.status.idle":"2022-04-07T15:48:26.858619Z","shell.execute_reply.started":"2022-04-07T15:48:10.868284Z","shell.execute_reply":"2022-04-07T15:48:26.854173Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"CPU times: user 634 ms, sys: 3.37 s, total: 4 s\nWall time: 15.9 s\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"   investment_id  time_id       f_0       f_1       f_2       f_3       f_4  \\\n0              1        0  0.932617  0.113708 -0.402100  0.378418 -0.203979   \n1              2        0  0.811035 -0.514160  0.742188 -0.616699 -0.194214   \n2              6        0  0.394043  0.615723  0.567871 -0.607910  0.068909   \n3              7        0 -2.343750 -0.011871  1.875000 -0.606445 -0.586914   \n4              8        0  0.842285 -0.262939  2.330078 -0.583496 -0.618164   \n\n        f_5       f_6       f_7  ...     f_291     f_292     f_293     f_294  \\\n0 -0.413574  0.965820  1.230469  ... -1.095703  0.200073  0.819336  0.941406   \n1  1.771484  1.427734  1.133789  ...  0.912598 -0.734375  0.819336  0.941406   \n2 -1.083008  0.979492 -1.125977  ...  0.912598 -0.551758 -1.220703 -1.060547   \n3 -0.815918  0.778320  0.299072  ...  0.912598 -0.266357 -1.220703  0.941406   \n4 -0.742676 -0.946777  1.230469  ...  0.912598 -0.741211 -1.220703  0.941406   \n\n      f_295     f_296     f_297     f_298     f_299    target  \n0 -0.086792 -1.086914 -1.044922 -0.287598  0.321533 -0.300781  \n1 -0.387695 -1.086914 -0.929688 -0.974121 -0.343506 -0.231079  \n2 -0.219116 -1.086914 -0.612305 -0.113953  0.243652  0.568848  \n3 -0.608887  0.104919 -0.783203  1.151367 -0.773438 -1.064453  \n4 -0.588379  0.104919  0.753418  1.345703 -0.737793 -0.531738  \n\n[5 rows x 303 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>investment_id</th>\n      <th>time_id</th>\n      <th>f_0</th>\n      <th>f_1</th>\n      <th>f_2</th>\n      <th>f_3</th>\n      <th>f_4</th>\n      <th>f_5</th>\n      <th>f_6</th>\n      <th>f_7</th>\n      <th>...</th>\n      <th>f_291</th>\n      <th>f_292</th>\n      <th>f_293</th>\n      <th>f_294</th>\n      <th>f_295</th>\n      <th>f_296</th>\n      <th>f_297</th>\n      <th>f_298</th>\n      <th>f_299</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0.932617</td>\n      <td>0.113708</td>\n      <td>-0.402100</td>\n      <td>0.378418</td>\n      <td>-0.203979</td>\n      <td>-0.413574</td>\n      <td>0.965820</td>\n      <td>1.230469</td>\n      <td>...</td>\n      <td>-1.095703</td>\n      <td>0.200073</td>\n      <td>0.819336</td>\n      <td>0.941406</td>\n      <td>-0.086792</td>\n      <td>-1.086914</td>\n      <td>-1.044922</td>\n      <td>-0.287598</td>\n      <td>0.321533</td>\n      <td>-0.300781</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0</td>\n      <td>0.811035</td>\n      <td>-0.514160</td>\n      <td>0.742188</td>\n      <td>-0.616699</td>\n      <td>-0.194214</td>\n      <td>1.771484</td>\n      <td>1.427734</td>\n      <td>1.133789</td>\n      <td>...</td>\n      <td>0.912598</td>\n      <td>-0.734375</td>\n      <td>0.819336</td>\n      <td>0.941406</td>\n      <td>-0.387695</td>\n      <td>-1.086914</td>\n      <td>-0.929688</td>\n      <td>-0.974121</td>\n      <td>-0.343506</td>\n      <td>-0.231079</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>6</td>\n      <td>0</td>\n      <td>0.394043</td>\n      <td>0.615723</td>\n      <td>0.567871</td>\n      <td>-0.607910</td>\n      <td>0.068909</td>\n      <td>-1.083008</td>\n      <td>0.979492</td>\n      <td>-1.125977</td>\n      <td>...</td>\n      <td>0.912598</td>\n      <td>-0.551758</td>\n      <td>-1.220703</td>\n      <td>-1.060547</td>\n      <td>-0.219116</td>\n      <td>-1.086914</td>\n      <td>-0.612305</td>\n      <td>-0.113953</td>\n      <td>0.243652</td>\n      <td>0.568848</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7</td>\n      <td>0</td>\n      <td>-2.343750</td>\n      <td>-0.011871</td>\n      <td>1.875000</td>\n      <td>-0.606445</td>\n      <td>-0.586914</td>\n      <td>-0.815918</td>\n      <td>0.778320</td>\n      <td>0.299072</td>\n      <td>...</td>\n      <td>0.912598</td>\n      <td>-0.266357</td>\n      <td>-1.220703</td>\n      <td>0.941406</td>\n      <td>-0.608887</td>\n      <td>0.104919</td>\n      <td>-0.783203</td>\n      <td>1.151367</td>\n      <td>-0.773438</td>\n      <td>-1.064453</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>8</td>\n      <td>0</td>\n      <td>0.842285</td>\n      <td>-0.262939</td>\n      <td>2.330078</td>\n      <td>-0.583496</td>\n      <td>-0.618164</td>\n      <td>-0.742676</td>\n      <td>-0.946777</td>\n      <td>1.230469</td>\n      <td>...</td>\n      <td>0.912598</td>\n      <td>-0.741211</td>\n      <td>-1.220703</td>\n      <td>0.941406</td>\n      <td>-0.588379</td>\n      <td>0.104919</td>\n      <td>0.753418</td>\n      <td>1.345703</td>\n      <td>-0.737793</td>\n      <td>-0.531738</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 303 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"investment_id = train.pop(\"investment_id\")\ninvestment_id.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-07T15:48:26.861094Z","iopub.execute_input":"2022-04-07T15:48:26.861722Z","iopub.status.idle":"2022-04-07T15:48:26.885983Z","shell.execute_reply.started":"2022-04-07T15:48:26.861677Z","shell.execute_reply":"2022-04-07T15:48:26.885421Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"0    1\n1    2\n2    6\n3    7\n4    8\nName: investment_id, dtype: uint16"},"metadata":{}}]},{"cell_type":"code","source":"_ = train.pop(\"time_id\")\ny = train.pop(\"target\")\ny.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-07T15:48:26.887112Z","iopub.execute_input":"2022-04-07T15:48:26.887919Z","iopub.status.idle":"2022-04-07T15:48:26.907023Z","shell.execute_reply.started":"2022-04-07T15:48:26.887876Z","shell.execute_reply":"2022-04-07T15:48:26.906185Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"0   -0.300781\n1   -0.231079\n2    0.568848\n3   -1.064453\n4   -0.531738\nName: target, dtype: float16"},"metadata":{}}]},{"cell_type":"code","source":"%%time\ninvestment_ids = list(investment_id.unique())\ninvestment_id_size = len(investment_ids) + 1\ninvestment_id_lookup_layer = layers.IntegerLookup(max_tokens=investment_id_size)\nwith tf.device(\"cpu\"):\n    investment_id_lookup_layer.adapt(investment_id)","metadata":{"execution":{"iopub.status.busy":"2022-04-07T15:48:26.909009Z","iopub.execute_input":"2022-04-07T15:48:26.909280Z","iopub.status.idle":"2022-04-07T15:49:06.924046Z","shell.execute_reply.started":"2022-04-07T15:48:26.909250Z","shell.execute_reply":"2022-04-07T15:49:06.921697Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"2022-04-07 15:48:26.991747: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n2022-04-07 15:48:27.133049: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n","output_type":"stream"},{"name":"stdout","text":"CPU times: user 54.4 s, sys: 8 s, total: 1min 2s\nWall time: 40 s\n","output_type":"stream"}]},{"cell_type":"code","source":"def preprocess(X, y):\n    print(X)\n    print(y)\n    return X, y\ndef make_dataset(feature, investment_id, y, batch_size=1024, mode=\"train\"):\n    ds = tf.data.Dataset.from_tensor_slices(((investment_id, feature), y))\n    ds = ds.map(preprocess)\n    if mode == \"train\":\n        ds = ds.shuffle(256)\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n    return ds","metadata":{"execution":{"iopub.status.busy":"2022-04-07T15:49:06.926589Z","iopub.execute_input":"2022-04-07T15:49:06.926930Z","iopub.status.idle":"2022-04-07T15:49:06.934097Z","shell.execute_reply.started":"2022-04-07T15:49:06.926886Z","shell.execute_reply":"2022-04-07T15:49:06.933185Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def get_model():\n    investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n    \n    investment_id_x = investment_id_lookup_layer(investment_id_inputs)\n    investment_id_x = layers.Embedding(investment_id_size, 32, input_length=1)(investment_id_x)\n    investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    \n    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n    feature_x = layers.Dense(256, activation='swish')(feature_x)\n    feature_x = layers.Dense(256, activation='swish')(feature_x)\n    \n    x = layers.Concatenate(axis=1)([investment_id_x, feature_x])\n    x = layers.Dense(512, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n    output = layers.Dense(1)(x)\n    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n    model = tf.keras.Model(inputs=[investment_id_inputs, features_inputs], outputs=[output])\n    model.compile(optimizer=tf.optimizers.Adam(0.001), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse])\n    return model\n\n\ndef get_model2():\n    investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n    \n    investment_id_x = investment_id_lookup_layer(investment_id_inputs)\n    investment_id_x = layers.Embedding(investment_id_size, 32, input_length=1)(investment_id_x)\n    investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)    \n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n   # investment_id_x = layers.Dropout(0.65)(investment_id_x)\n   \n    \n    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n    feature_x = layers.Dense(256, activation='swish')(feature_x)\n    feature_x = layers.Dense(256, activation='swish')(feature_x)\n    feature_x = layers.Dense(256, activation='swish')(feature_x)\n    feature_x = layers.Dropout(0.65)(feature_x)\n    \n    x = layers.Concatenate(axis=1)([investment_id_x, feature_x])\n    x = layers.Dense(512, activation='swish', kernel_regularizer=\"l2\")(x)\n   # x = layers.Dropout(0.2)(x)\n    x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n  #  x = layers.Dropout(0.4)(x)\n    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(0.75)(x)\n    output = layers.Dense(1)(x)\n    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n    model = tf.keras.Model(inputs=[investment_id_inputs, features_inputs], outputs=[output])\n    model.compile(optimizer=tf.optimizers.Adam(0.001), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse])\n    return model\n\n\ndef get_model3():\n    investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n    features_inputs = tf.keras.Input((300, ), dtype=tf.float32)\n    \n    investment_id_x = investment_id_lookup_layer(investment_id_inputs)\n    investment_id_x = layers.Embedding(investment_id_size, 32, input_length=1)(investment_id_x)\n    investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dropout(0.5)(investment_id_x)\n    investment_id_x = layers.Dense(32, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dropout(0.5)(investment_id_x)\n    #investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    \n    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n    feature_x = layers.Dropout(0.5)(feature_x)\n    feature_x = layers.Dense(128, activation='swish')(feature_x)\n    feature_x = layers.Dropout(0.5)(feature_x)\n    feature_x = layers.Dense(64, activation='swish')(feature_x)\n    \n    x = layers.Concatenate(axis=1)([investment_id_x, feature_x])\n    x = layers.Dropout(0.5)(x)\n    x = layers.Dense(64, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(0.5)(x)\n    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(0.5)(x)\n    x = layers.Dense(16, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(0.5)(x)\n    output = layers.Dense(1)(x)\n    output = tf.keras.layers.BatchNormalization(axis=1)(output)\n    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n    model = tf.keras.Model(inputs=[investment_id_inputs, features_inputs], outputs=[output])\n    model.compile(optimizer=tf.optimizers.Adam(0.001), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse])\n    return model\n\ndef get_model5():\n    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n    \n    ## feature ##\n    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n    feature_x = layers.Dropout(0.1)(feature_x)\n    ## convolution 1 ##\n    feature_x = layers.Reshape((-1,1))(feature_x)\n    feature_x = layers.Conv1D(filters=16, kernel_size=4, strides=1, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n    ## convolution 2 ##\n    feature_x = layers.Conv1D(filters=16, kernel_size=4, strides=4, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n    ## convolution 3 ##\n    feature_x = layers.Conv1D(filters=64, kernel_size=4, strides=1, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n    ## convolution 4 ##\n    feature_x = layers.Conv1D(filters=64, kernel_size=4, strides=4, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n    ## convolution 5 ##\n    feature_x = layers.Conv1D(filters=64, kernel_size=4, strides=2, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n    ## flatten ##\n    feature_x = layers.Flatten()(feature_x)\n    \n    x = layers.Dense(512, activation='swish', kernel_regularizer=\"l2\")(feature_x)\n    \n    x = layers.Dropout(0.1)(x)\n    x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(0.1)(x)\n    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(0.1)(x)\n    output = layers.Dense(1)(x)\n    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n    model = tf.keras.Model(inputs=[features_inputs], outputs=[output])\n    model.compile(optimizer=tf.optimizers.Adam(0.001), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse])\n    return model\n\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-04-07T15:49:06.935434Z","iopub.execute_input":"2022-04-07T15:49:06.935652Z","iopub.status.idle":"2022-04-07T15:49:07.289267Z","shell.execute_reply.started":"2022-04-07T15:49:06.935624Z","shell.execute_reply":"2022-04-07T15:49:07.288427Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"776"},"metadata":{}}]},{"cell_type":"code","source":"models = []\n\nfor i in range(5):\n    model = get_model()\n    model.load_weights(f'../input/dnn-base/model_{i}')\n    models.append(model)\n\nfor i in range(10):\n    model = get_model2()\n    model.load_weights(f'../input/train-dnn-v2-10fold/model_{i}')\n    models.append(model)\n    \n    \nfor i in range(10):\n    model = get_model3()\n    model.load_weights(f'../input/dnnmodelnew/model_{i}')\n    models.append(model)\n    \n    \nmodels2 = []\n    \nfor i in range(5):\n    model = get_model5()\n    model.load_weights(f'../input/prediction-including-spatial-info-with-conv1d/model_{i}.tf')\n    models2.append(model)","metadata":{"execution":{"iopub.status.busy":"2022-04-07T15:49:07.290414Z","iopub.execute_input":"2022-04-07T15:49:07.290668Z","iopub.status.idle":"2022-04-07T15:49:18.692322Z","shell.execute_reply.started":"2022-04-07T15:49:07.290609Z","shell.execute_reply":"2022-04-07T15:49:18.691548Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"2022-04-07 15:49:07.458670: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/dnn-base/model_0: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n2022-04-07 15:49:08.006216: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/dnn-base/model_1: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n2022-04-07 15:49:08.453680: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/dnn-base/model_2: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n2022-04-07 15:49:08.777822: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/dnn-base/model_3: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n2022-04-07 15:49:09.093113: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/dnn-base/model_4: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n2022-04-07 15:49:09.455324: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/train-dnn-v2-10fold/model_0: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n2022-04-07 15:49:09.850603: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/train-dnn-v2-10fold/model_1: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n2022-04-07 15:49:10.230167: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/train-dnn-v2-10fold/model_2: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n2022-04-07 15:49:10.616524: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/train-dnn-v2-10fold/model_3: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n2022-04-07 15:49:11.071217: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/train-dnn-v2-10fold/model_4: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n2022-04-07 15:49:11.496922: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/train-dnn-v2-10fold/model_5: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n2022-04-07 15:49:11.902019: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/train-dnn-v2-10fold/model_6: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n2022-04-07 15:49:12.322955: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/train-dnn-v2-10fold/model_7: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n2022-04-07 15:49:12.735711: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/train-dnn-v2-10fold/model_8: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n2022-04-07 15:49:13.152467: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/train-dnn-v2-10fold/model_9: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n2022-04-07 15:49:13.535110: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/dnnmodelnew/model_0: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n2022-04-07 15:49:13.833336: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/dnnmodelnew/model_1: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n2022-04-07 15:49:14.147930: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/dnnmodelnew/model_2: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n2022-04-07 15:49:14.449548: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/dnnmodelnew/model_3: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n2022-04-07 15:49:14.745113: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/dnnmodelnew/model_4: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n2022-04-07 15:49:15.032728: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/dnnmodelnew/model_5: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n2022-04-07 15:49:15.512855: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/dnnmodelnew/model_6: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n2022-04-07 15:49:15.796770: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/dnnmodelnew/model_7: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n2022-04-07 15:49:16.076853: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/dnnmodelnew/model_8: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n2022-04-07 15:49:16.384726: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/dnnmodelnew/model_9: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_model_dr04():\n    features_inputs = tf.keras.Input((300, ), dtype=tf.float32)\n    \n    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n    feature_x = layers.Dropout(0.4)(feature_x)\n    feature_x = layers.Dense(128, activation='swish')(feature_x)\n    feature_x = layers.Dropout(0.4)(feature_x)\n    feature_x = layers.Dense(64, activation='swish')(feature_x)\n    \n    x = layers.Concatenate(axis=1)([feature_x])\n    x = layers.Dropout(0.4)(x)\n    x = layers.Dense(64, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(0.4)(x)\n    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(0.4)(x)\n    x = layers.Dense(16, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(0.4)(x)\n    output = layers.Dense(1)(x)\n    output = tf.keras.layers.BatchNormalization(axis=1)(output)\n    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n    model = tf.keras.Model(inputs=[features_inputs], outputs=[output])\n    model.compile(optimizer=tf.optimizers.Adam(0.001),  loss = correlationLoss, metrics=[correlationMetric])\n    return model\n\ndr=0.3\n\ngpus = tf.config.experimental.list_physical_devices('GPU')\nfor gpu in gpus:\n    print(\"Name:\", gpu.name, \"  Type:\", gpu.device_type)\n\nn_features = 300\nfeatures = [f'f_{i}' for i in range(n_features)]\n\ndef preprocess(X, y):\n    return X, y\ndef make_dataset(feature, y, batch_size=1024, mode=\"train\"):\n    ds = tf.data.Dataset.from_tensor_slices((feature, y))\n    ds = ds.map(preprocess)\n    if mode == \"train\":\n        ds = ds.shuffle(512)\n#     ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n    ds = ds.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n    return ds\n\ndef correlationMetric(x, y, axis=-2):\n    \"\"\"Metric returning the Pearson correlation coefficient of two tensors over some axis, default -2.\"\"\"\n    x = tf.convert_to_tensor(x)\n    y = math_ops.cast(y, x.dtype)\n    n = tf.cast(tf.shape(x)[axis], x.dtype)\n    xsum = tf.reduce_sum(x, axis=axis)\n    ysum = tf.reduce_sum(y, axis=axis)\n    xmean = xsum / n\n    ymean = ysum / n\n    xvar = tf.reduce_sum( tf.math.squared_difference(x, xmean), axis=axis)\n    yvar = tf.reduce_sum( tf.math.squared_difference(y, ymean), axis=axis)\n    cov = tf.reduce_sum( (x - xmean) * (y - ymean), axis=axis)\n    corr = cov / tf.sqrt(xvar * yvar)\n    return tf.constant(1.0, dtype=x.dtype) - corr\n\n\ndef correlationLoss(x,y, axis=-2):\n    \"\"\"Loss function that maximizes the pearson correlation coefficient between the predicted values and the labels,\n    while trying to have the same mean and variance\"\"\"\n    x = tf.convert_to_tensor(x)\n    y = math_ops.cast(y, x.dtype)\n    n = tf.cast(tf.shape(x)[axis], x.dtype)\n    xsum = tf.reduce_sum(x, axis=axis)\n    ysum = tf.reduce_sum(y, axis=axis)\n    xmean = xsum / n\n    ymean = ysum / n\n    xsqsum = tf.reduce_sum( tf.math.squared_difference(x, xmean), axis=axis)\n    ysqsum = tf.reduce_sum( tf.math.squared_difference(y, ymean), axis=axis)\n    cov = tf.reduce_sum( (x - xmean) * (y - ymean), axis=axis)\n    corr = cov / tf.sqrt(xsqsum * ysqsum)\n    return tf.convert_to_tensor( K.mean(tf.constant(1.0, dtype=x.dtype) - corr ) , dtype=tf.float32 )\n\ndef correlationMetric_01mse(x, y, axis=-2):\n    \"\"\"Metric returning the Pearson correlation coefficient of two tensors over some axis, default -2.\"\"\"\n    x = tf.convert_to_tensor(x)\n    y = math_ops.cast(y, x.dtype)\n    n = tf.cast(tf.shape(x)[axis], x.dtype)\n    xsum = tf.reduce_sum(x, axis=axis)\n    ysum = tf.reduce_sum(y, axis=axis)\n    xmean = xsum / n\n    ymean = ysum / n\n    xvar = tf.reduce_sum( tf.math.squared_difference(x, xmean), axis=axis)\n    yvar = tf.reduce_sum( tf.math.squared_difference(y, ymean), axis=axis)\n    cov = tf.reduce_sum( (x - xmean) * (y - ymean), axis=axis)\n    corr = cov / tf.sqrt(xvar * yvar)\n    return tf.constant(1.0, dtype=x.dtype) - corr\n\ngc.collect()\n\n# list(GroupKFold(5).split(train , groups = train.index))[0]\ndef pearson_coef(data):\n    return data.corr()['target']['preds']\n\ndef evaluate_metric(valid_df):\n    return np.mean(valid_df[['time_id_', 'target', 'preds']].groupby('time_id').apply(pearson_coef))\n\n \nmodels3 = []\n\nfor index in range(10):\n    model = get_model_dr04()\n    model.load_weights(f\"../input/model10mse/model_{index}\")\n    models3.append(model)\n    \n    \n","metadata":{"execution":{"iopub.status.busy":"2022-04-07T15:49:18.694488Z","iopub.execute_input":"2022-04-07T15:49:18.695063Z","iopub.status.idle":"2022-04-07T15:49:20.982743Z","shell.execute_reply.started":"2022-04-07T15:49:18.695011Z","shell.execute_reply":"2022-04-07T15:49:20.981856Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"investment_id[~investment_id.isin([85, 905, 2558, 3662, 2800, 1415])]","metadata":{"execution":{"iopub.status.busy":"2022-04-07T15:49:20.985070Z","iopub.execute_input":"2022-04-07T15:49:20.985693Z","iopub.status.idle":"2022-04-07T15:49:21.054157Z","shell.execute_reply.started":"2022-04-07T15:49:20.985646Z","shell.execute_reply":"2022-04-07T15:49:21.053353Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"0             1\n1             2\n2             6\n3             7\n4             8\n           ... \n3141405    3768\n3141406    3769\n3141407    3770\n3141408    3772\n3141409    3773\nName: investment_id, Length: 3141374, dtype: uint16"},"metadata":{}}]},{"cell_type":"code","source":"investment_id2 = investment_id[~investment_id.isin([85, 905, 2558, 3662, 2800, 1415])]\n\ninvestment_ids2 = list(investment_id2.unique())\ninvestment_id_size2 = len(investment_ids2) + 1\ninvestment_id_lookup_layer2 = layers.IntegerLookup(max_tokens=investment_id_size2)\ninvestment_id_lookup_layer2.adapt(pd.DataFrame({\"investment_ids\":investment_ids}))","metadata":{"execution":{"iopub.status.busy":"2022-04-07T15:49:21.055345Z","iopub.execute_input":"2022-04-07T15:49:21.055548Z","iopub.status.idle":"2022-04-07T15:49:21.250980Z","shell.execute_reply.started":"2022-04-07T15:49:21.055524Z","shell.execute_reply":"2022-04-07T15:49:21.250141Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"\ndef get_model_best(ft_units, x_units, x_dropout):\n    investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n    \n    investment_id_x = investment_id_lookup_layer2(investment_id_inputs)\n    investment_id_x = layers.Embedding(investment_id_size2, 32, input_length=1)(investment_id_x)\n    investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n    investment_id_x = layers.Dense(128, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dense(128, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dense(128, activation='swish')(investment_id_x)\n    \n    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n    for hu in ft_units:\n        feature_x = layers.Dense(hu, activation='swish')(feature_x)\n    \n    x = layers.Concatenate(axis=1)([investment_id_x, feature_x])\n    \n    for i in range(len(x_units)):\n        x = tf.keras.layers.Dense(x_units[i], kernel_regularizer=\"l2\")(x) #v8\n        x = tf.keras.layers.BatchNormalization()(x) #v7\n        x = tf.keras.layers.Activation('swish')(x) #v7\n        x = tf.keras.layers.Dropout(x_dropout[i])(x) #v8\n        \n    output = layers.Dense(1)(x)\n    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n    model = tf.keras.Model(inputs=[investment_id_inputs, features_inputs], outputs=[output])\n    model.compile(optimizer=tf.optimizers.Adam(0.0001), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse])\n    return model\n\nparams = {\n    'ft_units': [256,256],\n    'x_units': [512, 256, 128, 32],\n    'x_dropout': [0.4, 0.3, 0.2, 0.1]\n#           'lr':1e-3, \n         }\n\nmodels_best = []\nscores = []\nfor i in range(7):\n    model = get_model_best(**params)\n    model.load_weights(f\"../input/wmodels/best/model_{i}.tf\")\n    models_best.append(model)","metadata":{"execution":{"iopub.status.busy":"2022-04-07T15:49:21.252740Z","iopub.execute_input":"2022-04-07T15:49:21.253002Z","iopub.status.idle":"2022-04-07T15:49:24.519345Z","shell.execute_reply.started":"2022-04-07T15:49:21.252964Z","shell.execute_reply":"2022-04-07T15:49:24.518460Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def get_model_corr(ft_units, x_units, x_dropout):\n    \n    # investment_id\n    investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n    investment_id_x = investment_id_lookup_layer(investment_id_inputs)\n    investment_id_x = layers.Embedding(investment_id_size, 32, input_length=1)(investment_id_x)\n    investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n    investment_id_x = layers.Dense(128, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dense(128, activation='swish')(investment_id_x) \n    investment_id_x = layers.Dense(128, activation='swish')(investment_id_x)\n    \n    # features_inputs\n    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n    bn = tf.keras.layers.BatchNormalization()(features_inputs)\n    gn = tf.keras.layers.GaussianNoise(0.035)(bn)\n    feature_x = layers.Dense(300, activation='swish')(gn)\n    feature_x = tf.keras.layers.Dropout(0.5)(feature_x)\n    \n    for hu in ft_units:\n        feature_x = layers.Dense(hu, activation='swish')(feature_x)\n#         feature_x = tf.keras.layers.Activation('swish')(feature_x)\n        feature_x = tf.keras.layers.Dropout(0.35)(feature_x)\n    \n    x = layers.Concatenate(axis=1)([investment_id_x, feature_x])\n    \n    for i in range(len(x_units)):\n        x = tf.keras.layers.Dense(x_units[i], kernel_regularizer=\"l2\")(x) \n        x = tf.keras.layers.Activation('swish')(x)\n        x = tf.keras.layers.Dropout(x_dropout[i])(x)\n        \n    output = layers.Dense(1)(x)\n    model = tf.keras.Model(inputs=[investment_id_inputs, features_inputs], outputs=[output])\n    model.compile(optimizer=tf.optimizers.Adam(0.0001), loss=correlationLoss, \n                  metrics=['mse', \"mae\", correlation])\n    return model\n\n\nparams = {\n#     'num_columns': len(features), \n    'ft_units': [150, 75, 150 ,200],\n    'x_units': [512, 256, 128, 32],\n    'x_dropout': [0.44, 0.4, 0.33, 0.2] #4, 3, 2, 1\n#           'lr':1e-3, \n         }","metadata":{"execution":{"iopub.status.busy":"2022-04-07T15:49:24.520989Z","iopub.execute_input":"2022-04-07T15:49:24.521514Z","iopub.status.idle":"2022-04-07T15:49:24.533394Z","shell.execute_reply.started":"2022-04-07T15:49:24.521465Z","shell.execute_reply":"2022-04-07T15:49:24.532826Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"del train\ndel investment_id\ndel y","metadata":{"execution":{"iopub.status.busy":"2022-04-07T15:49:24.535227Z","iopub.execute_input":"2022-04-07T15:49:24.535583Z","iopub.status.idle":"2022-04-07T15:49:24.550189Z","shell.execute_reply.started":"2022-04-07T15:49:24.535556Z","shell.execute_reply":"2022-04-07T15:49:24.549284Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def preprocess_test(investment_id, feature):\n    return (investment_id, feature), 0\n\ndef preprocess_test_s(feature):\n    return (feature), 0\n\ndef make_test_dataset(feature, investment_id, batch_size=1024):\n    ds = tf.data.Dataset.from_tensor_slices(((investment_id, feature)))\n    ds = ds.map(preprocess_test)\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n    return ds\n\ndef make_test_dataset2(feature, batch_size=1024):\n    ds = tf.data.Dataset.from_tensor_slices(((feature)))\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.AUTOTUNE)\n    return ds\n\ndef inference(models, ds):\n    y_preds = []\n    for model in models:\n        y_pred = model.predict(ds)\n        y_preds.append(y_pred)\n    return np.mean(y_preds, axis=0)\n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=1)\n\n\ndef make_test_dataset3(feature, batch_size=1024):\n    ds = tf.data.Dataset.from_tensor_slices((feature))\n    ds = ds.map(preprocess_test_s)\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n    return ds\n\ndef infer(models, ds):\n    y_preds = []\n    for model in models:\n        y_pred = model.predict(ds)\n        y_preds.append((y_pred-y_pred.mean())/y_pred.std())\n    \n    return np.mean(y_preds, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-04-07T15:49:24.551520Z","iopub.execute_input":"2022-04-07T15:49:24.552034Z","iopub.status.idle":"2022-04-07T15:49:24.586812Z","shell.execute_reply.started":"2022-04-07T15:49:24.551997Z","shell.execute_reply":"2022-04-07T15:49:24.586164Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"import ubiquant\nenv = ubiquant.make_env()\niter_test = env.iter_test() \nfor (test_df, sample_prediction_df) in iter_test:\n    ds = make_test_dataset(test_df[features], test_df[\"investment_id\"])\n    p1 = inference(models, ds)\n    ds2 = make_test_dataset2(test_df[features])\n    p2 = inference(models2, ds2)\n    ds3 = make_test_dataset3(test_df[features])\n    p3 = infer(models3, ds3)\n    p4 = inference(models_best, ds)\n    sample_prediction_df['target'] = p1 * 0.3 + p2 * 0.61 + p3 * 0.09\n    env.predict(sample_prediction_df) \n    display(sample_prediction_df)","metadata":{"execution":{"iopub.status.busy":"2022-04-07T15:49:24.587914Z","iopub.execute_input":"2022-04-07T15:49:24.588165Z","iopub.status.idle":"2022-04-07T15:49:37.297704Z","shell.execute_reply.started":"2022-04-07T15:49:24.588113Z","shell.execute_reply":"2022-04-07T15:49:37.296962Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   row_id    target\n0  1220_1 -0.129613\n1  1220_2  0.029042","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>row_id</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1220_1</td>\n      <td>-0.129613</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1220_2</td>\n      <td>0.029042</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"   row_id    target\n0  1221_0 -0.072325\n1  1221_1 -0.083487\n2  1221_2 -0.133640","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>row_id</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1221_0</td>\n      <td>-0.072325</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1221_1</td>\n      <td>-0.083487</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1221_2</td>\n      <td>-0.133640</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"   row_id    target\n0  1222_0 -0.095639\n1  1222_1 -0.049287\n2  1222_2 -0.069021","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>row_id</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1222_0</td>\n      <td>-0.095639</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1222_1</td>\n      <td>-0.049287</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1222_2</td>\n      <td>-0.069021</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:39: RuntimeWarning: invalid value encountered in true_divide\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   row_id  target\n0  1223_0     NaN","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>row_id</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1223_0</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]}]}